{"cells":[{"cell_type":"markdown","metadata":{},"source":["# FHV Trip Records\n","\n","\n","In this notebook, we will perform the ETL process for the [FHV Trip Records](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)."]},{"cell_type":"markdown","metadata":{},"source":["**Obs.:** To perform the data assessment, we will use the [Data Dictionary â€“ FHV Trip Records](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf), provided by the TCL NYC Website. In this document, we will check the description of each field name, keeping in mind the possible values and range of values for each data field."]},{"cell_type":"markdown","metadata":{},"source":["## Step 1: Import Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datetime\n","import pyspark.sql.functions as f\n","from pyspark.sql.types import IntegerType"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: Load the Data\n","\n","Since the data has the same schema, we can easily perform: \n","\n","```spark.read()```\n","\n","And pass the folder to it:"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["df = spark.read.parquet(\"gs://mobilab-tech-task-bucket/fhv\")"]},{"cell_type":"markdown","metadata":{},"source":["We will also define the begin and the current year for future analysis"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["begin = 2020\n","\n","now = datetime.datetime.now()\n","until = now.year"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Exploratory Data Analysis\n","\n","In order to get to know our data, we will perform a basic exploratory analysis of it:"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There is 39535386 rows in the dataframe\n"]}],"source":["print(f\"There is {df.count()} rows in the dataframe\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- dispatching_base_num: string (nullable = true)\n"," |-- pickup_datetime: timestamp (nullable = true)\n"," |-- dropOff_datetime: timestamp (nullable = true)\n"," |-- PUlocationID: double (nullable = true)\n"," |-- DOlocationID: double (nullable = true)\n"," |-- SR_Flag: integer (nullable = true)\n"," |-- Affiliated_base_number: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+--------------------+-------+\n","|summary|dispatching_base_num|SR_Flag|\n","+-------+--------------------+-------+\n","|  count|            39535386|      0|\n","|   mean|                null|   null|\n","| stddev|                null|   null|\n","|    min|              B00001|   null|\n","|    max|              b03340|   null|\n","+-------+--------------------+-------+\n","\n"]}],"source":["df.select('dispatching_base_num', 'SR_Flag').describe().show()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+-------------------+\n","|    pickup_datetime|   dropOff_datetime|\n","+-------------------+-------------------+\n","|2020-01-01 00:30:00|2020-01-01 01:44:00|\n","|2020-01-01 00:30:00|2020-01-01 00:47:00|\n","|2020-01-01 00:48:00|2020-01-01 01:19:00|\n","|2020-01-01 00:34:00|2020-01-01 00:43:00|\n","|2020-01-01 00:23:00|2020-01-01 00:32:00|\n","|2020-01-01 00:52:00|2020-01-01 01:01:00|\n","|2020-01-01 00:20:30|2020-01-01 00:45:52|\n","|2020-01-01 00:08:15|2020-01-01 00:12:03|\n","|2020-01-01 00:40:30|2020-01-01 01:06:23|\n","|2020-01-01 00:53:04|2020-01-01 01:19:13|\n","+-------------------+-------------------+\n","only showing top 10 rows\n","\n"]}],"source":["df.select('pickup_datetime','dropOff_datetime').show(10)"]},{"cell_type":"markdown","metadata":{},"source":["**Issues**\n","\n","*   `Null` values besides the `SR_Flag` column\n","\n","**Possible Issues**\n","\n","*   `dropOff_datetime` > 2022\n","* `passenger_count` < 0\n","* `payment_type` out of the range\n","* `pickup_datetime` < 2022\n","* `dropOff_datetime` - `pickup_datetime` < 0\n","\n","We will handle with this issues in the transformation step."]},{"cell_type":"markdown","metadata":{},"source":["## Step 4: Data Transformation\n","\n","Here, we will perform a series of data transformation methods, such as filtering, type conversion, row dropping, etc. Focus on building a more robust dataset."]},{"cell_type":"markdown","metadata":{},"source":["**Time Period**\n","\n","**Pickup Datetime**\n","\n","- 1.0 Checking wheater the `lpep_pickup_datetime` is in the range of years previously defined."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+\n","|    pickup_datetime|\n","+-------------------+\n","|2020-01-01 00:00:00|\n","|2020-01-01 00:00:00|\n","|2020-01-01 00:00:02|\n","|2020-01-01 00:00:07|\n","|2020-01-01 00:00:07|\n","+-------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('pickup_datetime').sort(f.col(\"pickup_datetime\")).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["- 1.1 - Dropping the out of range rows"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"year\", f.year(f.col(\"pickup_datetime\")))"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["df = df.filter(f'year >= {begin} and year <= {until}')\n","df = df.drop('year')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+\n","|    pickup_datetime|\n","+-------------------+\n","|2020-01-01 00:00:00|\n","|2020-01-01 00:00:00|\n","|2020-01-01 00:00:02|\n","|2020-01-01 00:00:07|\n","|2020-01-01 00:00:07|\n","+-------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('pickup_datetime').sort(f.col(\"pickup_datetime\")).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["**Dropoff Datetime**\n","\n","- 2.0 Checking wheater the `lpep_dropoff_datetime` is in the range of years previously defined."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+\n","|   dropOff_datetime|\n","+-------------------+\n","|2050-02-05 11:30:00|\n","|2031-05-01 16:00:04|\n","|2028-07-28 17:50:00|\n","|2027-07-01 06:00:00|\n","|2024-12-01 14:30:18|\n","+-------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('dropOff_datetime').sort(f.col(\"dropOff_datetime\").desc()).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["In this particular case, the data frame already meets the requirement, but we will implement the filter thinking in future use cases."]},{"cell_type":"markdown","metadata":{},"source":["- 2.1 - Dropping the out of range rows"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"year\", f.year(f.col(\"dropOff_datetime\")))"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["df = df.filter(f'year >= {begin} and year <= {until}')\n","df = df.drop('year')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+\n","|   dropOff_datetime|\n","+-------------------+\n","|2022-12-30 16:00:00|\n","|2022-12-28 19:50:00|\n","|2022-12-28 18:45:00|\n","|2022-12-28 14:30:00|\n","|2022-12-27 16:30:00|\n","+-------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('dropOff_datetime').sort(f.col(\"dropOff_datetime\").desc()).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["**Timestamps Analysis**\n","\n","The difference between `lpep_dropoff_datetime` and `lpep_pickup_datetime` must be greater than zero.\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["df = df.withColumn('DiffInSeconds', f.unix_timestamp(\"dropOff_datetime\") - f.unix_timestamp('pickup_datetime'))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["df = df.filter('DiffInSeconds > 0')\n","df = df.drop('DiffInSeconds')"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1: Timestamp Requirement\n","\n","Since the data science team wants to evaluate data also based on the hours and the day of the week, we could define two extra columns in our dataset.\n","\n","Our date and time values are already in a timestamp type, so it will be a quick transformation that will save the time of our team in the future."]},{"cell_type":"markdown","metadata":{},"source":["**Hours**\n","\n","On 24-hour time format."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"pickup_hour\", f.hour(f.col(\"pickup_datetime\"))) \\\n","       .withColumn(\"dropOff_hour\", f.hour(f.col(\"dropOff_datetime\")))"]},{"cell_type":"markdown","metadata":{},"source":["**Day of the week**\n","\n","This transformation will generate a column with the first three letters of the respective day of the week based on the timestamps."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"pickup_day\", f.date_format('pickup_datetime', 'E')) \\\n","       .withColumn(\"dropoff_day\", f.date_format('dropOff_datetime', 'E'))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Step 5: Data Schema Check\n","\n","First, let us take a look on the actual data schema:"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- dispatching_base_num: string (nullable = true)\n"," |-- pickup_datetime: timestamp (nullable = true)\n"," |-- dropOff_datetime: timestamp (nullable = true)\n"," |-- PUlocationID: double (nullable = true)\n"," |-- DOlocationID: double (nullable = true)\n"," |-- SR_Flag: integer (nullable = true)\n"," |-- Affiliated_base_number: string (nullable = true)\n"," |-- pickup_hour: integer (nullable = true)\n"," |-- dropOff_hour: integer (nullable = true)\n"," |-- pickup_day: string (nullable = true)\n"," |-- dropoff_day: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["The schema is nice defined."]},{"cell_type":"markdown","metadata":{},"source":["## Step 6: Data Transformation Check\n","\n","Now, we will perform the same exploratory data analysis that before, in order to evaluate the results of the data transformation step.\n","\n","The goal here is to confirm that we dealt properly with the spotted issues."]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+--------------------+-------+\n","|summary|dispatching_base_num|SR_Flag|\n","+-------+--------------------+-------+\n","|  count|            39535379|      0|\n","|   mean|                null|   null|\n","| stddev|                null|   null|\n","|    min|              B00001|   null|\n","|    max|              b03340|   null|\n","+-------+--------------------+-------+\n","\n"]}],"source":["df.select('dispatching_base_num', 'SR_Flag').describe().show()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 39535379 rows in the transformed data frame\n"]}],"source":["print(f\"There are {df.count()} rows in the transformed data frame\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 7: Outputs\n","\n","As the pipeline requirements, defined by our data science team, the output datasets are required in:\n","\n","1. **Colum-oriented format**\n","2. **Row-oriented format**\n","3. **Delta lake format**\n","\n","Since we are working in the Google Cloud (GC) platform, to meet the requirements will use the GC resources:\n","\n","## Step 7: Outputs\n","\n","As the pipeline requirements, defined by our data science team, the output datasets are required in:\n","\n","1. **Colum-oriented format**\n","2. **Row-oriented format**\n","3. **Delta lake format**\n","\n","Since we are working in the Google Cloud (GC) platform, to meet the requirements will use the GC resources:\n","\n","1. **Colum-oriented format**\n","\n","     **No available due to versioning issues.**\n","\n","\n","2. **Row-oriented format**\n","\n","     **No available due to versioning issues.**\n","\n","In the above two topics, PySpark was not able to generate the files due to a versioning error. This error only appears in the Yellow Taxi Trip data, and the FHV Trip data. The Green and FHVHV data, even with the same function, do not have this problem.\n","\n","\n","3. **Delta lake format**\n","\n","     **No available due to versioning issues.**"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}
