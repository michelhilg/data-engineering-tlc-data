{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Green Taxi Trip Records\n","\n","\n","In this notebook, we will perform the ETL process for the [Green Taxi Trip Records](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)."]},{"cell_type":"markdown","metadata":{},"source":["**Obs.:** To perform the data assessment, we will use the [Data Dictionary â€“ Green Taxi Trip Records](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf), provided by the TCL NYC Website. In this document, we will check the description of each field name, keeping in mind the possible values and range of values for each data field."]},{"cell_type":"markdown","metadata":{},"source":["## Step 1: Import Dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import datetime\n","import pyspark.sql.functions as f\n","from pyspark.sql.types import IntegerType"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: Load the Data\n","\n","Since the data has the same schema, we can easily perform: \n","\n","```spark.read()```\n","\n","And pass the folder to it:"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["df = spark.read.parquet(\"gs://tlc-data-task/green-taxi\")"]},{"cell_type":"markdown","metadata":{},"source":["We will also define the begin and the current year for future analysis"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["begin = 2020\n","\n","now = datetime.datetime.now()\n","until = now.year"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Exploratory Data Analysis\n","\n","In order to get to know our data, we will perform a basic exploratory analysis of it:"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 3370228 rows in the data frame\n"]}],"source":["print(f\"There are {df.count()} rows in the data frame\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- VendorID: long (nullable = true)\n"," |-- lpep_pickup_datetime: timestamp (nullable = true)\n"," |-- lpep_dropoff_datetime: timestamp (nullable = true)\n"," |-- store_and_fwd_flag: string (nullable = true)\n"," |-- RatecodeID: double (nullable = true)\n"," |-- PULocationID: long (nullable = true)\n"," |-- DOLocationID: long (nullable = true)\n"," |-- passenger_count: double (nullable = true)\n"," |-- trip_distance: double (nullable = true)\n"," |-- fare_amount: double (nullable = true)\n"," |-- extra: double (nullable = true)\n"," |-- mta_tax: double (nullable = true)\n"," |-- tip_amount: double (nullable = true)\n"," |-- tolls_amount: double (nullable = true)\n"," |-- ehail_fee: integer (nullable = true)\n"," |-- improvement_surcharge: double (nullable = true)\n"," |-- total_amount: double (nullable = true)\n"," |-- payment_type: double (nullable = true)\n"," |-- trip_type: double (nullable = true)\n"," |-- congestion_surcharge: double (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+------------------+------------------+-----------------+------------------+------------------+------------------+\n","|summary|   passenger_count|     trip_distance|     total_amount|      payment_type|        tip_amount|         trip_type|\n","+-------+------------------+------------------+-----------------+------------------+------------------+------------------+\n","|  count|           2366544|           3370228|          3370228|           2366544|           3370228|           2366533|\n","|   mean|1.2826940889330603| 70.45998095084565|21.15086389113421|1.4293070401395453| 1.344507695028476|1.0334062529447086|\n","| stddev|0.9282045229762351|2811.1694111225156| 16.2021957291594|0.5170671378178895|2.5232576853447246| 0.179694988392713|\n","|    min|               0.0|            -33.69|           -300.8|               1.0|             -86.0|               1.0|\n","|    max|              48.0|         360068.14|          2113.55|               5.0|             641.2|               2.0|\n","+-------+------------------+------------------+-----------------+------------------+------------------+------------------+\n","\n"]}],"source":["df.select('passenger_count','trip_distance', 'total_amount', 'payment_type', 'tip_amount', 'trip_type').describe().show()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+---------------------+\n","|lpep_pickup_datetime|lpep_dropoff_datetime|\n","+--------------------+---------------------+\n","| 2019-12-18 15:52:30|  2019-12-18 15:54:39|\n","| 2020-01-01 00:45:58|  2020-01-01 00:56:39|\n","| 2020-01-01 00:41:38|  2020-01-01 00:52:49|\n","| 2020-01-01 00:52:46|  2020-01-01 01:14:21|\n","| 2020-01-01 00:19:57|  2020-01-01 00:30:56|\n","| 2020-01-01 00:52:33|  2020-01-01 01:09:54|\n","| 2020-01-01 00:10:18|  2020-01-01 00:22:16|\n","| 2020-01-01 01:03:14|  2020-01-01 01:29:45|\n","| 2020-01-01 00:04:11|  2020-01-01 00:09:48|\n","| 2020-01-01 00:25:52|  2020-01-01 00:32:16|\n","+--------------------+---------------------+\n","only showing top 10 rows\n","\n"]}],"source":["df.select('lpep_pickup_datetime','lpep_dropoff_datetime').show(10)"]},{"cell_type":"markdown","metadata":{},"source":["**Issues**\n","\n","*   `trip_distance` < 0\n","*   `total_amount` < 0\n","*   `tip_amount` < 0\n","*   `passenger_count` > 10\n","* `lpep_pickup_datetime` < 2020\n","\n","**Possible Issues**\n","\n","* `passenger_count` < 0\n","* `payment_type` out of the range\n","* `lpep_dropoff_datetime` > 2022\n","* `lpep_dropoff_datetime` - `lpep_dropoff_datetime` < 0\n","\n","We will handle with this issues in the transformation step."]},{"cell_type":"markdown","metadata":{},"source":["## Step 4: Data Transformation\n","\n","Here, we will perform a series of data transformation methods, such as filtering, type conversion, row dropping, etc. Focus on building a more robust dataset."]},{"cell_type":"markdown","metadata":{},"source":["**Trip Distance Filtering**"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["df = df.filter('trip_distance >= 0')"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df.filter('trip_distance < 0').count()"]},{"cell_type":"markdown","metadata":{},"source":["**Passenger Count Filtering**"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|max(passenger_count)|\n","+--------------------+\n","|                 9.0|\n","+--------------------+\n","\n"]}],"source":["df = df.filter('passenger_count <= 10')\n","df.agg({'passenger_count': 'max' }).show()"]},{"cell_type":"markdown","metadata":{},"source":["**Total Amount Filtering**"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------+\n","|min(total_amount)|\n","+-----------------+\n","|              0.0|\n","+-----------------+\n","\n"]}],"source":["df = df.filter('total_amount >= 0')\n","df.agg({'total_amount': 'min' }).show()"]},{"cell_type":"markdown","metadata":{},"source":["**Tip Amount Filtering**"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+\n","|min(tip_amount)|\n","+---------------+\n","|            0.0|\n","+---------------+\n","\n"]}],"source":["df = df.filter('tip_amount >= 0')\n","df.agg({'tip_amount': 'min' }).show()"]},{"cell_type":"markdown","metadata":{},"source":["**Payment Type Analysis**\n","\n","Checkin wheater the payment is consistent with the total amount or not."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+------------+\n","|total_amount|payment_type|\n","+------------+------------+\n","|         0.0|         4.0|\n","|         0.0|         1.0|\n","|         0.0|         4.0|\n","|         0.0|         2.0|\n","|         0.0|         2.0|\n","|         0.0|         2.0|\n","|         0.0|         2.0|\n","|         0.0|         2.0|\n","|         0.0|         4.0|\n","|         0.0|         2.0|\n","+------------+------------+\n","only showing top 10 rows\n","\n"]}],"source":["df.select('total_amount','payment_type').filter('total_amount == 0 and payment_type != 3').show(10)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["7097"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df.filter('total_amount == 0 and payment_type != 3').count()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["df = df.withColumn(\"payment_type\", f.when(df[\"total_amount\"] == 0, 3).otherwise(df[\"payment_type\"]))\n","\n","df.filter('total_amount == 0 and payment_type != 3').count()"]},{"cell_type":"markdown","metadata":{},"source":["Replacing `0` value as `Nan` in `payment_type`, since is out of the range."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["0"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df = df.withColumn(\"payment_type\", f.when(df[\"payment_type\"] == 0, np.nan).otherwise(df[\"payment_type\"]))\n","df.filter('payment_type == 0').count()"]},{"cell_type":"markdown","metadata":{},"source":["**Trip Type Analysis**\n","\n","Checking wheater the trip type is on the range defined in the dictionary."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------+\n","|trip_type|\n","+---------+\n","|     null|\n","|      1.0|\n","|      2.0|\n","+---------+\n","\n"]}],"source":["df.select(\"trip_type\").distinct().show()"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------+\n","|count(CASE WHEN (trip_type IS NULL) THEN true END)|\n","+--------------------------------------------------+\n","|                                                11|\n","+--------------------------------------------------+\n","\n"]}],"source":["df.select([f.count(f.when(f.col('trip_type').isNull(),True))]).show()"]},{"cell_type":"markdown","metadata":{},"source":["We will deal with these `Null` values at the end, writing a general rule for the dataset."]},{"cell_type":"markdown","metadata":{},"source":["**Time Period**\n","\n","**Pickup Datetime**\n","\n","- 1.0 Checking wheater the `lpep_pickup_datetime` is in the range of years previously defined."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|lpep_pickup_datetime|\n","+--------------------+\n","| 2008-12-31 17:04:15|\n","| 2008-12-31 19:16:53|\n","| 2008-12-31 22:06:48|\n","| 2008-12-31 23:05:21|\n","| 2008-12-31 23:05:26|\n","+--------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('lpep_pickup_datetime').sort(f.col(\"lpep_pickup_datetime\")).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["- 1.1 - Dropping the out of range rows"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"year\", f.year(f.col(\"lpep_pickup_datetime\")))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["df = df.filter(f'year >= {begin} and year <= {until}')\n","df = df.drop('year')"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|lpep_pickup_datetime|\n","+--------------------+\n","| 2020-01-01 00:00:07|\n","| 2020-01-01 00:00:21|\n","| 2020-01-01 00:00:44|\n","| 2020-01-01 00:01:04|\n","| 2020-01-01 00:01:11|\n","+--------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('lpep_pickup_datetime').sort(f.col(\"lpep_pickup_datetime\")).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["**Dropoff Datetime**\n","\n","- 2.0 Checking wheater the `lpep_dropoff_datetime` is in the range of years previously defined."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------+\n","|lpep_dropoff_datetime|\n","+---------------------+\n","|  2022-09-01 16:47:53|\n","|  2022-09-01 09:13:18|\n","|  2022-09-01 08:56:55|\n","|  2022-09-01 08:02:58|\n","|  2022-09-01 05:14:10|\n","+---------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('lpep_dropoff_datetime').sort(f.col(\"lpep_dropoff_datetime\").desc()).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["In this particular case, the data frame already meets the requirement, but we will implement the filter thinking in future use cases."]},{"cell_type":"markdown","metadata":{},"source":["- 2.1 - Dropping the out of range rows"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"year\", f.year(f.col(\"lpep_dropoff_datetime\")))"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["df = df.filter(f'year >= {begin} and year <= {until}')\n","df = df.drop('year')"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------+\n","|lpep_dropoff_datetime|\n","+---------------------+\n","|  2022-09-01 16:47:53|\n","|  2022-09-01 09:13:18|\n","|  2022-09-01 08:56:55|\n","|  2022-09-01 08:02:58|\n","|  2022-09-01 05:14:10|\n","+---------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('lpep_dropoff_datetime').sort(f.col(\"lpep_dropoff_datetime\").desc()).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["**Timestamps Analysis**\n","\n","The difference between `lpep_dropoff_datetime` and `lpep_pickup_datetime` must be greater than zero.\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["df = df.withColumn('DiffInSeconds', f.unix_timestamp(\"lpep_dropoff_datetime\") - f.unix_timestamp('lpep_pickup_datetime'))"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["df = df.filter('DiffInSeconds > 0')\n","df = df.drop('DiffInSeconds')"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1: Timestamp Requirement\n","\n","Since the data science team wants to evaluate data also based on the hours and the day of the week, we could define two extra columns in our dataset.\n","\n","Our date and time values are already in a timestamp type, so it will be a quick transformation that will save the time of our team in the future."]},{"cell_type":"markdown","metadata":{},"source":["**Hours**\n","\n","On 24-hour time format."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"lpep_pickup_hour\", f.hour(f.col(\"lpep_pickup_datetime\"))) \\\n","       .withColumn(\"lpep_dropoff_hour\", f.hour(f.col(\"lpep_dropoff_datetime\")))"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------+-----------------+\n","|lpep_pickup_hour|lpep_dropoff_hour|\n","+----------------+-----------------+\n","|               0|                0|\n","|               0|                0|\n","|               0|                1|\n","|               0|                0|\n","|               0|                1|\n","+----------------+-----------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('lpep_pickup_hour', 'lpep_dropoff_hour').show(5)"]},{"cell_type":"markdown","metadata":{},"source":["**Day of the week**\n","\n","This transformation will generate a column with the first three letters of the respective day of the week based on the timestamps."]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"pickup_day\", f.date_format('lpep_pickup_datetime', 'E')) \\\n","       .withColumn(\"dropoff_day\", f.date_format('lpep_dropoff_datetime', 'E'))\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------+----------+---------------------+-----------+\n","|lpep_dropoff_datetime|pickup_day|lpep_dropoff_datetime|dropoff_day|\n","+---------------------+----------+---------------------+-----------+\n","|  2020-01-01 00:56:39|       Wed|  2020-01-01 00:56:39|        Wed|\n","|  2020-01-01 00:52:49|       Wed|  2020-01-01 00:52:49|        Wed|\n","|  2020-01-01 01:14:21|       Wed|  2020-01-01 01:14:21|        Wed|\n","|  2020-01-01 00:30:56|       Wed|  2020-01-01 00:30:56|        Wed|\n","|  2020-01-01 01:09:54|       Wed|  2020-01-01 01:09:54|        Wed|\n","+---------------------+----------+---------------------+-----------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('lpep_dropoff_datetime','pickup_day', 'lpep_dropoff_datetime', 'dropoff_day').show(5)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 5: Data Schema Check\n","\n","First, let us take a look on the actual data schema:"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- VendorID: long (nullable = true)\n"," |-- lpep_pickup_datetime: timestamp (nullable = true)\n"," |-- lpep_dropoff_datetime: timestamp (nullable = true)\n"," |-- store_and_fwd_flag: string (nullable = true)\n"," |-- RatecodeID: double (nullable = true)\n"," |-- PULocationID: long (nullable = true)\n"," |-- DOLocationID: long (nullable = true)\n"," |-- passenger_count: double (nullable = true)\n"," |-- trip_distance: double (nullable = true)\n"," |-- fare_amount: double (nullable = true)\n"," |-- extra: double (nullable = true)\n"," |-- mta_tax: double (nullable = true)\n"," |-- tip_amount: double (nullable = true)\n"," |-- tolls_amount: double (nullable = true)\n"," |-- ehail_fee: integer (nullable = true)\n"," |-- improvement_surcharge: double (nullable = true)\n"," |-- total_amount: double (nullable = true)\n"," |-- payment_type: double (nullable = true)\n"," |-- trip_type: double (nullable = true)\n"," |-- congestion_surcharge: double (nullable = true)\n"," |-- lpep_pickup_hour: integer (nullable = true)\n"," |-- lpep_dropoff_hour: integer (nullable = true)\n"," |-- pickup_day: string (nullable = true)\n"," |-- dropoff_day: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["Some data types above are weird, for example: `passenger_count` as a `double` (float).\n","\n","In the code cell below, we better define some data types for the final dataset schema."]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- VendorID: integer (nullable = true)\n"," |-- lpep_pickup_datetime: timestamp (nullable = true)\n"," |-- lpep_dropoff_datetime: timestamp (nullable = true)\n"," |-- store_and_fwd_flag: string (nullable = true)\n"," |-- RatecodeID: double (nullable = true)\n"," |-- PULocationID: long (nullable = true)\n"," |-- DOLocationID: long (nullable = true)\n"," |-- passenger_count: integer (nullable = true)\n"," |-- trip_distance: double (nullable = true)\n"," |-- fare_amount: double (nullable = true)\n"," |-- extra: double (nullable = true)\n"," |-- mta_tax: double (nullable = true)\n"," |-- tip_amount: double (nullable = true)\n"," |-- tolls_amount: double (nullable = true)\n"," |-- ehail_fee: integer (nullable = true)\n"," |-- improvement_surcharge: double (nullable = true)\n"," |-- total_amount: double (nullable = true)\n"," |-- payment_type: integer (nullable = true)\n"," |-- trip_type: integer (nullable = true)\n"," |-- congestion_surcharge: double (nullable = true)\n"," |-- lpep_pickup_hour: integer (nullable = true)\n"," |-- lpep_dropoff_hour: integer (nullable = true)\n"," |-- pickup_day: string (nullable = true)\n"," |-- dropoff_day: string (nullable = true)\n","\n"]}],"source":["df = df \\\n",".withColumn(\"VendorID\" ,df[\"VendorID\"].cast(IntegerType())) \\\n",".withColumn(\"passenger_count\" ,df[\"passenger_count\"].cast(IntegerType())) \\\n",".withColumn(\"payment_type\",df[\"payment_type\"].cast(IntegerType())) \\\n",".withColumn(\"trip_type\" ,df[\"trip_type\"].cast(IntegerType()))\n","\n","df.printSchema()"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+\n","|passenger_count|\n","+---------------+\n","|              1|\n","|              6|\n","|              3|\n","|              5|\n","|              9|\n","|              4|\n","|              8|\n","|              7|\n","|              2|\n","|              0|\n","+---------------+\n","\n"]}],"source":["df.select('passenger_count').distinct().show(100)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 6: Data Transformation Check\n","\n","Now, we will perform the same exploratory data analysis that before, in order to evaluate the results of the data transformation step.\n","\n","The goal here is to confirm that we dealt properly with the spotted issues."]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+------------------+------------------+------------------+------------------+------------------+------------------+\n","|summary|   passenger_count|     trip_distance|      total_amount|      payment_type|        tip_amount|         trip_type|\n","+-------+------------------+------------------+------------------+------------------+------------------+------------------+\n","|  count|           2355539|           2355539|           2355539|           2355539|           2355539|           2355538|\n","|   mean|1.2826486846534912|3.5308017358232973|17.140716269415297|1.4267991317486146|1.3911471726857954|1.0331334073150167|\n","| stddev|   0.9277478304743|237.77371887603908|14.889035858848855|0.5133153338395617|2.6982737601472744|0.1789849106344823|\n","|    min|                 0|               0.0|               0.0|                 1|               0.0|                 1|\n","|    max|                 9|         244152.01|           2113.55|                 5|             641.2|                 2|\n","+-------+------------------+------------------+------------------+------------------+------------------+------------------+\n","\n"]}],"source":["df.select('passenger_count','trip_distance', 'total_amount', 'payment_type', 'tip_amount', 'trip_type').describe().show()"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+---------------------+\n","|lpep_pickup_datetime|lpep_dropoff_datetime|\n","+--------------------+---------------------+\n","| 2020-01-01 00:45:58|  2020-01-01 00:56:39|\n","| 2020-01-01 00:41:38|  2020-01-01 00:52:49|\n","| 2020-01-01 00:52:46|  2020-01-01 01:14:21|\n","| 2020-01-01 00:19:57|  2020-01-01 00:30:56|\n","| 2020-01-01 00:52:33|  2020-01-01 01:09:54|\n","+--------------------+---------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('lpep_pickup_datetime','lpep_dropoff_datetime').show(5)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 2355539 rows in the transformed data frame\n"]}],"source":["print(f\"There are {df.count()} rows in the transformed data frame\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Step 7: Outputs\n","\n","As the pipeline definition, the format of the output datasets are:\n","\n","1. **Colum-oriented format**\n","2. **Row-oriented format**\n","\n","Since we are working in the Google Cloud Platform (GCP), to meet the requirements will use the GC resources:\n","\n","1. **Colum-oriented format**\n","\n","    Export to Google Cloud Storage as a `.parquet` files. After, load the files as a Big Query table. Google Big Query storage is a solution for column-oriented databases. You could more info on [Overview of BigQuery storage](https://cloud.google.com/bigquery/docs/storage_overview)\n","\n","\n","2. **Row-oriented format**\n","\n","    Export to Google Cloud Storage as a `.csv` files. The `.csv` files is the standard for row-oriented databases, this files could be uploaded lately in a SQL solution (e.g: MySQL, Postgres, or even Google Cloud SQL)."]},{"cell_type":"markdown","metadata":{},"source":["**1. Colum-oriented format**\n","\n","Since the data does not have a  vehicle ID identification, we select the passenger count as the partition column. The reason is that this column does not have as many distinct values as the others, as the result we will generate a manageable number of files."]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["df.write.mode('overwrite').format('parquet').partitionBy('passenger_count').save('gs://tlc-data-task/outputs/green_trip/parquet')"]},{"cell_type":"markdown","metadata":{},"source":["**2. Row-oriented format**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.write.mode('overwrite').format('csv').partitionBy('passenger_count').save('gs://tlc-data-task/outputs/green_trip/csv')"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}
