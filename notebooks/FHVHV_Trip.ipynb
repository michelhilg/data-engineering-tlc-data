{"cells":[{"cell_type":"markdown","metadata":{},"source":["# FHVHV Trip Records\n","\n","\n","In this notebook, we will perform the ETL process for the [FHVHV Trip Records](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)."]},{"cell_type":"markdown","metadata":{},"source":["**Obs.:** To perform the data assessment, we will use the [Data Dictionary â€“ FHVHV Trip Records](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf), provided by the TCL NYC Website. In this document, we will check the description of each field name, keeping in mind the possible values and range of values for each data field."]},{"cell_type":"markdown","metadata":{},"source":["## Step 1: Import Dependencies"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import datetime\n","import pyspark.sql.functions as f\n","from pyspark.sql.types import IntegerType"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2: Load the Data\n","\n","Since the data has the same schema, we can easily perform: \n","\n","```spark.read()```\n","\n","And pass the folder to it:"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df = spark.read.parquet(\"gs://mobilab-tech-task-bucket/fhvhv\")"]},{"cell_type":"markdown","metadata":{},"source":["We will also define the begin and the current year for future analysis"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["begin = 2020\n","\n","now = datetime.datetime.now()\n","until = now.year"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3: Exploratory Data Analysis\n","\n","In order to get to know our data, we will perform a basic exploratory analysis of it:"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There is 455471222 rows in the dataframe\n"]}],"source":["print(f\"There is {df.count()} rows in the dataframe\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- hvfhs_license_num: string (nullable = true)\n"," |-- dispatching_base_num: string (nullable = true)\n"," |-- originating_base_num: string (nullable = true)\n"," |-- request_datetime: timestamp (nullable = true)\n"," |-- on_scene_datetime: timestamp (nullable = true)\n"," |-- pickup_datetime: timestamp (nullable = true)\n"," |-- dropoff_datetime: timestamp (nullable = true)\n"," |-- PULocationID: long (nullable = true)\n"," |-- DOLocationID: long (nullable = true)\n"," |-- trip_miles: double (nullable = true)\n"," |-- trip_time: long (nullable = true)\n"," |-- base_passenger_fare: double (nullable = true)\n"," |-- tolls: double (nullable = true)\n"," |-- bcf: double (nullable = true)\n"," |-- sales_tax: double (nullable = true)\n"," |-- congestion_surcharge: double (nullable = true)\n"," |-- airport_fee: double (nullable = true)\n"," |-- tips: double (nullable = true)\n"," |-- driver_pay: double (nullable = true)\n"," |-- shared_request_flag: string (nullable = true)\n"," |-- shared_match_flag: string (nullable = true)\n"," |-- access_a_ride_flag: string (nullable = true)\n"," |-- wav_request_flag: string (nullable = true)\n"," |-- wav_match_flag: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------------+------------------+------------------+------------------+-------------------+\n","|summary|       trip_miles|         trip_time|              tips|        driver_pay|base_passenger_fare|\n","+-------+-----------------+------------------+------------------+------------------+-------------------+\n","|  count|        455471222|         455471222|         455471222|         455471222|          455471222|\n","|   mean|4.819631009109297|1088.5584195350107|0.8036569993439668|16.794573631012756| 21.004440467214035|\n","| stddev|5.512223049853493| 765.2270550604832|2.5864894824143256|13.804533881142962|  17.45281175653547|\n","|    min|              0.0|                 0|               0.0|          -2035.92|            -520.11|\n","|    max|          1310.51|            240764|            1000.0|           4894.62|            8157.74|\n","+-------+-----------------+------------------+------------------+------------------+-------------------+\n","\n"]}],"source":["df.select('trip_miles','trip_time', 'tips', 'driver_pay', 'base_passenger_fare').describe().show()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+-------------------+\n","|    pickup_datetime|   dropoff_datetime|\n","+-------------------+-------------------+\n","|2020-01-01 00:45:34|2020-01-01 01:02:20|\n","|2020-01-01 00:47:50|2020-01-01 00:53:23|\n","|2020-01-01 00:04:37|2020-01-01 00:21:49|\n","|2020-01-01 00:26:36|2020-01-01 00:33:00|\n","|2020-01-01 00:37:49|2020-01-01 00:46:59|\n","|2020-01-01 00:49:23|2020-01-01 01:07:26|\n","|2020-01-01 00:21:11|2020-01-01 00:36:58|\n","|2020-01-01 00:38:28|2020-01-01 00:42:38|\n","|2020-01-01 00:46:26|2020-01-01 01:09:55|\n","|2020-01-01 00:15:35|2020-01-01 00:23:21|\n","+-------------------+-------------------+\n","only showing top 10 rows\n","\n"]}],"source":["df.select('pickup_datetime','dropoff_datetime').show(10)"]},{"cell_type":"markdown","metadata":{},"source":["**Issues**\n","\n","*   `driver_pay` < 0\n","*   `base_passenger_fare` < 0\n","\n","**Possible Issues**\n","\n","* `pickup_datetime` < 2020\n","* `dropoff_datetime` > 2022\n","* `dropoff_datetime` - `pickup_datetime` < 0\n","* `pickup_datetime` - `request_datetime` < 0\n","* `boolean` variables ou of range\n","\n","We will handle with this issues in the transformation step."]},{"cell_type":"markdown","metadata":{},"source":["## Step 4: Data Transformation\n","\n","Here, we will perform a series of data transformation methods, such as filtering, type conversion, row dropping, etc. Focus on building a more robust dataset."]},{"cell_type":"markdown","metadata":{},"source":["**Driver Pay Filtering**"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["df = df.filter('driver_pay >= 0')"]},{"cell_type":"markdown","metadata":{},"source":["**Base Passenger Filtering**"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["df = df.filter('base_passenger_fare >= 0')"]},{"cell_type":"markdown","metadata":{},"source":["**Boolean Filtering**\n","\n","In total, we have 5 columns with boolean characteristics, we will perform a filter in the two most general ones. "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["df = df.filter(df.shared_match_flag.contains('N') | df.shared_match_flag.contains('Y'))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["df = df.filter(df.shared_request_flag.contains('N') | df.shared_request_flag.contains('Y'))"]},{"cell_type":"markdown","metadata":{},"source":["**Time Period**\n","\n","**Pickup Datetime**\n","\n","- 1.0 Checking wheater the `tpep_pickup_datetime` is in the range of years previously defined."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+\n","|    pickup_datetime|\n","+-------------------+\n","|2020-01-01 00:00:00|\n","|2020-01-01 00:00:00|\n","|2020-01-01 00:00:01|\n","|2020-01-01 00:00:01|\n","|2020-01-01 00:00:01|\n","+-------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('pickup_datetime').sort(f.col(\"pickup_datetime\")).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["- 1.1 - Dropping the out of range rows"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"year\", f.year(f.col(\"pickup_datetime\")))"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["df = df.filter(f'year >= {begin} and year <= {until}')\n","df = df.drop('year')"]},{"cell_type":"markdown","metadata":{},"source":["**Dropoff Datetime**\n","\n","- 2.0 Checking wheater the `tpep_dropoff_datetime` is in the range of years previously defined."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+\n","|   dropoff_datetime|\n","+-------------------+\n","|2022-09-01 04:37:02|\n","|2022-09-01 03:13:58|\n","|2022-09-01 02:42:08|\n","|2022-09-01 02:04:35|\n","|2022-09-01 01:46:23|\n","+-------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('dropoff_datetime').sort(f.col(\"dropoff_datetime\").desc()).show(5)"]},{"cell_type":"markdown","metadata":{},"source":["In this particular case, the data frame already meets the requirement, but we will implement the filter thinking in future use cases."]},{"cell_type":"markdown","metadata":{},"source":["- 2.1 - Dropping the out of range rows"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"year\", f.year(f.col(\"dropoff_datetime\")))"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["df = df.filter(f'year >= {begin} and year <= {until}')\n","df = df.drop('year')"]},{"cell_type":"markdown","metadata":{},"source":["**Timestamps Analysis**\n","\n","The difference between `dropoff_datetime` and `pickup_datetime` must be greater than zero.\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["df = df.withColumn('DiffInSeconds', f.unix_timestamp(\"dropoff_datetime\") - f.unix_timestamp('pickup_datetime'))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["df = df.filter('DiffInSeconds > 0')\n","df = df.drop('DiffInSeconds')"]},{"cell_type":"markdown","metadata":{},"source":["The difference between `pickup_datetime` and `request_datetime` must be greater than zero."]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["df = df.withColumn('DiffInSeconds', f.unix_timestamp(\"pickup_datetime\") - f.unix_timestamp('request_datetime'))"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["df = df.filter('DiffInSeconds > 0')\n","df = df.drop('DiffInSeconds')"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1: Timestamp Requirement\n","\n","Since the data science team wants to evaluate data also based on the hours and the day of the week, we could define two extra columns in our dataset.\n","\n","Our date and time values are already in a timestamp type, so it will be a quick transformation that will save the time of our team in the future."]},{"cell_type":"markdown","metadata":{},"source":["**Hours**\n","\n","On 24-hour time format."]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"pickup_hour\", f.hour(f.col(\"pickup_datetime\"))) \\\n","       .withColumn(\"dropoff_hour\", f.hour(f.col(\"dropoff_datetime\")))"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------------+\n","|pickup_hour|dropoff_hour|\n","+-----------+------------+\n","|          0|           1|\n","|          0|           0|\n","+-----------+------------+\n","only showing top 2 rows\n","\n"]}],"source":["df.select('pickup_hour', 'dropoff_hour').show(2)"]},{"cell_type":"markdown","metadata":{},"source":["**Day of the week**\n","\n","This transformation will generate a column with the first three letters of the respective day of the week based on the timestamps."]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"pickup_day\", f.date_format('pickup_datetime', 'E')) \\\n","       .withColumn(\"dropoff_day\", f.date_format('dropoff_datetime', 'E'))\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+----------+-------------------+-----------+\n","|    pickup_datetime|pickup_day|   dropoff_datetime|dropoff_day|\n","+-------------------+----------+-------------------+-----------+\n","|2020-01-01 00:45:34|       Wed|2020-01-01 01:02:20|        Wed|\n","|2020-01-01 00:47:50|       Wed|2020-01-01 00:53:23|        Wed|\n","+-------------------+----------+-------------------+-----------+\n","only showing top 2 rows\n","\n"]}],"source":["df.select('pickup_datetime','pickup_day', 'dropoff_datetime', 'dropoff_day').show(2)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 5: Data Schema Check\n","\n","First, let us take a look on the actual data schema:"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- hvfhs_license_num: string (nullable = true)\n"," |-- dispatching_base_num: string (nullable = true)\n"," |-- originating_base_num: string (nullable = true)\n"," |-- request_datetime: timestamp (nullable = true)\n"," |-- on_scene_datetime: timestamp (nullable = true)\n"," |-- pickup_datetime: timestamp (nullable = true)\n"," |-- dropoff_datetime: timestamp (nullable = true)\n"," |-- PULocationID: long (nullable = true)\n"," |-- DOLocationID: long (nullable = true)\n"," |-- trip_miles: double (nullable = true)\n"," |-- trip_time: long (nullable = true)\n"," |-- base_passenger_fare: double (nullable = true)\n"," |-- tolls: double (nullable = true)\n"," |-- bcf: double (nullable = true)\n"," |-- sales_tax: double (nullable = true)\n"," |-- congestion_surcharge: double (nullable = true)\n"," |-- airport_fee: double (nullable = true)\n"," |-- tips: double (nullable = true)\n"," |-- driver_pay: double (nullable = true)\n"," |-- shared_request_flag: string (nullable = true)\n"," |-- shared_match_flag: string (nullable = true)\n"," |-- access_a_ride_flag: string (nullable = true)\n"," |-- wav_request_flag: string (nullable = true)\n"," |-- wav_match_flag: string (nullable = true)\n"," |-- pickup_hour: integer (nullable = true)\n"," |-- dropoff_hour: integer (nullable = true)\n"," |-- pickup_day: string (nullable = true)\n"," |-- dropoff_day: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["The above original schema above make sense to represent the data."]},{"cell_type":"markdown","metadata":{},"source":["## Step 6: Data Transformation Check\n","\n","Now, we will perform the same exploratory data analysis that before, in order to evaluate the results of the data transformation step.\n","\n","The goal here is to confirm that we dealt properly with the spotted issues."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------+-----------------+------------------+------------------+------------------+-------------------+\n","|summary|       trip_miles|         trip_time|              tips|        driver_pay|base_passenger_fare|\n","+-------+-----------------+------------------+------------------+------------------+-------------------+\n","|  count|        451765224|         451765224|         451765224|         451765224|          451765224|\n","|   mean|4.790737384438591|1084.6164302721982|0.7939633281734142|16.713675021330857| 20.933809697964115|\n","| stddev|5.465292537316417| 758.6772573366525|   2.5577681447341|13.683363837334575|  17.29711410951984|\n","|    min|              0.0|                 0|               0.0|               0.0|                0.0|\n","|    max|           738.95|            147918|            1000.0|           4894.62|            8157.74|\n","+-------+-----------------+------------------+------------------+------------------+-------------------+\n","\n"]}],"source":["df.select('trip_miles','trip_time', 'tips', 'driver_pay', 'base_passenger_fare').describe().show()"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------+-------------------+\n","|    pickup_datetime|   dropoff_datetime|\n","+-------------------+-------------------+\n","|2020-01-01 00:45:34|2020-01-01 01:02:20|\n","|2020-01-01 00:47:50|2020-01-01 00:53:23|\n","|2020-01-01 00:04:37|2020-01-01 00:21:49|\n","|2020-01-01 00:26:36|2020-01-01 00:33:00|\n","|2020-01-01 00:37:49|2020-01-01 00:46:59|\n","+-------------------+-------------------+\n","only showing top 5 rows\n","\n"]}],"source":["df.select('pickup_datetime','dropoff_datetime').show(5)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 451765224 rows in the transformed dataframe\n"]}],"source":["print(f\"There are {df.count()} rows in the transformed dataframe\")"]},{"cell_type":"markdown","metadata":{},"source":["## Step 7: Outputs\n","\n","As the pipeline requirements, defined by our data science team, the output datasets are required in:\n","\n","1. **Colum-oriented format**\n","2. **Row-oriented format**\n","3. **Delta lake format**\n","\n","Since we are working in the Google Cloud (GC) platform, to meet the requirements will use the GC resources:\n","\n","1. **Colum-oriented format**\n","\n","    Export to Google Cloud Storage as a `.parquet` files. After, load the files as a Big Query table. Google Big Query storage is a solution for column-oriented databases. You could more info on [Overview of BigQuery storage](https://cloud.google.com/bigquery/docs/storage_overview)\n","\n","\n","2. **Row-oriented format**\n","\n","    Export to Google Cloud Storage as a `.csv` files. The `.csv` files is the standard for row-oriented databases, this files could be uploaded lately in a SQL solution (e.g: MySQL, Postgres, or even Google Cloud SQL).\n","\n","\n","3. **Delta lake format**\n","\n","    No available due to libraries issues."]},{"cell_type":"markdown","metadata":{},"source":["**1. Colum-oriented format**\n","\n","Since the data does not have a  vehicle ID identification, we select the pickup day as the partition column. The reason is that this column does not have as many distinct values as the others, as the result we will generate a manageable number of files."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.write.mode('overwrite').format('parquet').partitionBy('pickup_day').save('gs://mobilab-tech-task-bucket/outputs/fhvhv/parquet')"]},{"cell_type":"markdown","metadata":{},"source":["**2. Row-oriented format**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.write.mode('overwrite').format('csv').partitionBy('pickup_day').save('gs://mobilab-tech-task-bucket/outputs/fhvhv/csv')"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}